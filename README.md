# model-attention-analysis
Show which types of model attention are most sensitive to the number of attention heads and on which layers.
